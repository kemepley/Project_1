{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-167085006db2>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-167085006db2>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    return import pandas as pd\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def import_packages():\n",
    "\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    import statsmodels.api as sm\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from math import sqrt\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import Ridge\n",
    "    import seaborn as sns \n",
    "    \n",
    "    # warnings get old after awhile\n",
    "    warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that allows us to drop rows based on values we think are suspect\n",
    "\n",
    "def drop_rows(df, colname, val):\n",
    "    return df[df[colname] != val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to take columns with years (yr_built, yr_renovated) and bin them by decade\n",
    "\n",
    "def bin_by_decade(df, colname):\n",
    "    bins = pd.IntervalIndex.from_tuples([(1900, 1909), (1910, 1919), (1920, 1929), (1930, 1939), (1940, 1949), (1950, 1959), (1960, 1969), (1970, 1979), (1980, 1989), (1990, 1999), (2000, 2010), (2010, 2020)])\n",
    "    series = pd.cut(df[colname], bins)\n",
    "    df[colname] = series\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains all of our data cleaning operations\n",
    "\n",
    "def clean():\n",
    "    \n",
    "# Load the data\n",
    "    \n",
    "    house = pd.read_csv('https://raw.githubusercontent.com/learn-co-students/dsc-v2-mod1-final-project-dc-ds-career-042219/master/kc_house_data.csv')\n",
    "    \n",
    "# Get rid of rows\n",
    "    # The row with 33 bedrooms is a suspicious outlier, so we decided to drop it\n",
    "    # The 454 ?s in sqft_basement needed to be remedied. We chose to drop them. \n",
    "\n",
    "    clean_house = drop_rows(house, 'bedrooms', 33)\n",
    "    clean_house = drop_rows(clean_house,'sqft_basement', \"?\")\n",
    "    \n",
    "# Bin decades\n",
    "    \n",
    "    clean_house = bin_by_decade(clean_house, 'yr_built')\n",
    "    clean_house = bin_by_decade(clean_house, 'yr_renovated')\n",
    "    \n",
    "    \n",
    "# Get dummies\n",
    "    # Note: Originally, we dropped waterfront because it had a lot of null values. Get dummies gives you the option to create a dummy for nulls or not.\n",
    "    # We did that, without the null column\n",
    "    clean_house = pd.get_dummies(clean_house, columns=(['waterfront', 'view', 'floors', 'bedrooms', 'bathrooms','condition', 'grade','zipcode', 'yr_built', 'yr_renovated']))\n",
    "    \n",
    "# Drop id, date\n",
    "\n",
    "    clean_house = clean_house.drop(['id', 'date'], axis=1)\n",
    "    \n",
    "# Convert sqft_basement from object to float\n",
    "\n",
    "    clean_house['sqft_basement'] = clean_house['sqft_basement'].astype(float, inplace=True)\n",
    "    \n",
    "# Return clean_house\n",
    "    \n",
    "    return clean_house"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stepwise_selection(X, y, verbose = True)\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_house_predictors = clean_house.drop([\"price\"], axis=1)\n",
    "\n",
    "ss_scaler = preprocessing.StandardScaler()\n",
    "clean_house_ss = ss_scaler.fit_transform(clean_house_predictors)\n",
    "\n",
    "# Standard scaler returns a numpy array, so we converted it back to a data frame.\n",
    "\n",
    "clean_house_ss = pd.DataFrame(clean_house_ss, columns=list(clean_house_predictors.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the standardized data into train and test sets, this time using all of the columns from clean_house\n",
    "\n",
    "y = clean_house[\"price\"]\n",
    "X = clean_house_ss\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size=0.2, random_state=3)\n",
    "print(len(X_test), len(X_train), len(y_test), len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data to Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIt the data to Ridge\n",
    "\n",
    "ridge = Ridge()\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checked to see how many features were selected \n",
    "\n",
    "coeff_used = np.sum(lasso.coef_!=0)\n",
    "print(\"number of features used:\", coeff_used)\n",
    "print(\"number eliminated:\", len(list(clean_house.columns)) - coeff_used)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
